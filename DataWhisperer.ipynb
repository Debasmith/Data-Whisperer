{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be89f955-922b-4156-99f1-f465d7b00599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'crm_data' created successfully using path: 'file:///C:/Users/KIIT/Desktop/synthetic_crm_data.csv'\n",
      "Tables in database: [Table(name='crm_data', catalog='spark_catalog', namespace=['default'], description=None, tableType='EXTERNAL', isTemporary=False)]\n",
      "Successfully loaded data from table 'crm_data' into DataFrame.\n",
      "           Name                              Email                Phone                Company   Industry   Status LastContacted\n",
      "     John Lewis stevenblackburn@robbins-turner.com           1653970950            Santana LLC     Retail Prospect    2024-06-04\n",
      "  William Davis            mullinswesley@yahoo.com     740-058-6169x086         Rosario-Thomas Healthcare Customer    2025-03-04\n",
      "    Kelly Patel             alicejones@pittman.com   447-611-7401x94969 Young, White and Smith Technology     Lead    2024-11-21\n",
      "Chris Wilkerson          angelvillanueva@gmail.com         821.900.9018                 Wu PLC     Retail     Lead    2024-12-28\n",
      "   Sherry Small                    pcook@gmail.com 001-149-593-6688x348  Hall, Patel and Bauer    Finance Customer    2025-04-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Column: `Name` (Type: StringType()) - Sample distinct values: [Jeffrey Hunt, Kelly Patel, Linda Meyer, Pamela Young, Lori Ramirez]',\n",
       " 'Column: `Email` (Type: StringType()) - Sample distinct values: [kirbyarthur@fields-anderson.net, carrie25@jackson-christian.com, munozanthony@wilson-fuller.info, jjackson@wood.biz, melanie22@carrillo.com]',\n",
       " 'Column: `Phone` (Type: StringType()) - Sample distinct values: [001-280-768-7988x797, +1-345-140-1522x36173, (055)413-6228x641, 5888393372, +1-529-506-5957x6137]',\n",
       " 'Column: `Company` (Type: StringType()) - Sample distinct values: [King, Bailey and Berry, Hansen Group, Ali, Lee and Case, Pearson-Espinoza, Hernandez-Heath]',\n",
       " 'Column: `Industry` (Type: StringType()) - Sample distinct values: [Education, Healthcare, Finance, Technology, Retail]',\n",
       " 'Column: `Status` (Type: StringType()) - Sample distinct values: [Prospect, Lead, Customer, Churned]',\n",
       " 'Column: `LastContacted` (Type: DateType()) - Sample distinct values: [2025-02-16, 2024-06-04, 2024-10-02, 2025-02-06, 2025-02-12]']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CRM Data Analysis\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "file_path = r\"C:\\Users\\KIIT\\Desktop\\synthetic_crm_data.csv\"\n",
    "sql_friendly_path = file_path.replace(\"\\\\\", \"/\")\n",
    "\n",
    "if os.path.isabs(sql_friendly_path) and not sql_friendly_path.startswith(\"file:///\"):\n",
    "    sql_friendly_path = \"file:///\" + sql_friendly_path\n",
    "\n",
    "table_name = \"crm_data\"\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE {table_name}\n",
    "        USING csv\n",
    "        OPTIONS (path '{sql_friendly_path}', header 'true', inferSchema 'true')\n",
    "    \"\"\")\n",
    "    print(f\"Table '{table_name}' created successfully using path: '{sql_friendly_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating table '{table_name}': {e}\")\n",
    "    spark.stop()\n",
    "    exit()\n",
    "    \n",
    "print(f\"Tables in database: {spark.catalog.listTables()}\")\n",
    "\n",
    "try:\n",
    "    df = spark.table(table_name)\n",
    "    print(f\"Successfully loaded data from table '{table_name}' into DataFrame.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from table '{table_name}': {e}\")\n",
    "    spark.stop()\n",
    "    exit()\n",
    "\n",
    "schema_df = df.schema\n",
    "\n",
    "schema_details = []\n",
    "for field in schema_df.fields:\n",
    "    col_name = field.name\n",
    "    col_type_str = str(field.dataType)\n",
    "    \n",
    "    try:\n",
    "        sample_values_rows = spark.sql(f\"SELECT DISTINCT `{col_name}` FROM {table_name} LIMIT 5\").collect()\n",
    "        sample_values = [row[0] for row in sample_values_rows]\n",
    "        sample_str = \", \".join([str(val) for val in sample_values if val is not None])\n",
    "        schema_details.append(f\"Column: `{col_name}` (Type: {col_type_str}) - Sample distinct values: [{sample_str}]\")\n",
    "    except Exception as e:\n",
    "        schema_details.append(f\"Column: `{col_name}` (Type: {col_type_str}) - Error fetching sample values: {e}\")\n",
    "\n",
    "try:\n",
    "    head_df_pandas = df.limit(5).toPandas()\n",
    "    head_str = head_df_pandas.to_string(index=False)\n",
    "    print(head_str)\n",
    "except Exception as e:\n",
    "    print(f\"Error generating DataFrame head: {e}\")\n",
    "\n",
    "schema_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5093a4a-248f-4b89-ae9c-bce44fa166b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1:\n",
      "SQL: SELECT\n",
      "  Status,\n",
      "  COUNT(*) AS customer_count\n",
      "FROM crm_data\n",
      "GROUP BY\n",
      "  Status;\n",
      "\n",
      "DataFrame[Status: string, customer_count: bigint]\n",
      "\n",
      "Query 2:\n",
      "SQL: SELECT Industry, COUNT(*) AS customer_count\n",
      "FROM crm_data\n",
      "GROUP BY Industry;\n",
      "DataFrame[Industry: string, customer_count: bigint]\n",
      "\n",
      "Query 3:\n",
      "SQL: SELECT\n",
      "  Name\n",
      "FROM crm_data\n",
      "WHERE\n",
      "  Status = 'Churned';\n",
      "\n",
      "DataFrame[Name: string]\n",
      "\n",
      "Query 4:\n",
      "SQL: SELECT\n",
      "  *\n",
      "FROM crm_data\n",
      "WHERE\n",
      "  LastContacted < date_sub(current_date(), 60);\n",
      "DataFrame[Name: string, Email: string, Phone: string, Company: string, Industry: string, Status: string, LastContacted: date]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from openai import OpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import traceback\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama'\n",
    ")\n",
    "\n",
    "user_query = [\n",
    "    \"Count of customers by status\",\n",
    "    \"Count of customers by industry\",\n",
    "    \"List of all churned customers\",\n",
    "    \"Customers who haven't been contacted in the last 60 days\"\n",
    "]\n",
    "\n",
    "system_template = \"\"\"You are an expert SQL query generator specializing in Spark SQL syntax. \n",
    "Generate precise, optimized SQL queries based on the schema provided.\n",
    "Always follow these rules:\n",
    "1. Only return the SQL query with no explanations or markdown formatting\n",
    "2. For date calculations, use date_sub(current_date(), n) for subtracting days\n",
    "3. Do not use DATE('now'), NOW(), or MySQL/SQLite-style date functions\n",
    "4. Use simple capital letters for main query aliases\n",
    "5. Always reference the table as 'crm_data'\n",
    "6. Use proper date functions compatible with Spark SQL\n",
    "7. Never modify the schema\n",
    "8. Be consistent in your query generation approach\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "human_template = \"\"\"Based on the schema and sample data below, write a SQL query that answers this question: {user_query}\n",
    "\n",
    "SCHEMA DETAILS:\n",
    "{schema_str}\n",
    "\n",
    "SAMPLE DATA:\n",
    "{head_str}\n",
    "\n",
    "SQL QUERY:\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(human_template)\n",
    "])\n",
    "\n",
    "\n",
    "llm1 = ChatOpenAI(\n",
    "    model=\"gemma3\",\n",
    "    openai_api_base=\"http://localhost:11434/v1\",\n",
    "    openai_api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "result = chat_prompt | llm1 | parser\n",
    "\n",
    "\n",
    "def generate_query(user_queries):\n",
    "    sql_queries = []\n",
    "    for query in user_queries:\n",
    "        sql = result.invoke({\n",
    "            \"schema_str\": \"\\n\".join(schema_details),\n",
    "            \"head_str\": head_str,\n",
    "            \"user_query\": query\n",
    "        })\n",
    "        sql = sql.replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n",
    "        sql_queries.append(sql)\n",
    "    return sql_queries\n",
    "\n",
    "\n",
    "check_query_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Check if the following SQL query is valid for the given schema and Spark SQL. \n",
    "If any error is found, fix the query as per the schema and Spark SQL requirements.\n",
    "Return only the corrected SQL query, nothing else.\n",
    "\n",
    "Schema:\n",
    "{schema_str}\n",
    "\n",
    "SQL Query:\n",
    "{sql_query}\n",
    "\"\"\")\n",
    "\n",
    "checker_chain = check_query_prompt | llm1 | parser\n",
    "\n",
    "def check(sql_queries_tuple):\n",
    "    checked_queries = []\n",
    "    for query in sql_queries_tuple:\n",
    "        checked_sql = checker_chain.invoke({\n",
    "            \"schema_str\": \"\\n\".join(schema_details),\n",
    "            \"sql_query\": query\n",
    "        })\n",
    "        checked_queries.append(checked_sql)\n",
    "    return tuple(checked_queries)\n",
    "\n",
    "def execute_query_with_error_capture(sql_query, spark):\n",
    "    try:\n",
    "        result_df = spark.sql(sql_query)\n",
    "        return {\"success\": True, \"result\": result_df}\n",
    "    except Exception as e:\n",
    "        tb_str = traceback.format_exc()\n",
    "        return {\"success\": False, \"error_message\": str(e), \"traceback\": tb_str}\n",
    "\n",
    "error_fix_prompt = PromptTemplate.from_template(\"\"\"\n",
    "The following SQL query was executed in Spark SQL and resulted in an error.\n",
    "Please fix the query so it works, considering the schema and error message below.\n",
    "Return only the corrected SQL query, in Spark SQL syntax.\n",
    "\n",
    "Schema:\n",
    "{schema_str}\n",
    "\n",
    "SQL Query:\n",
    "{sql_query}\n",
    "\n",
    "Error Message:\n",
    "{error_message}\n",
    "\"\"\")\n",
    "\n",
    "def auto_fix_and_execute(sql_query, spark, schema_details, max_attempts=5):\n",
    "    attempts = 0\n",
    "    current_sql = sql_query\n",
    "    while attempts < max_attempts:\n",
    "        result = execute_query_with_error_capture(current_sql, spark)\n",
    "        if result[\"success\"]:\n",
    "            return result[\"result\"]\n",
    "        else:\n",
    "            corrected_chain = error_fix_prompt | llm1 | parser\n",
    "            current_sql = corrected_chain.invoke({\n",
    "                \"schema_str\": \"\\n\".join(schema_details),\n",
    "                \"sql_query\": current_sql,\n",
    "                \"error_message\": result[\"error_message\"]\n",
    "            })\n",
    "        attempts += 1\n",
    "    raise Exception(\"Failed to generate a correct SQL query after multiple attempts.\")\n",
    "\n",
    "def execute():\n",
    "    results = generate_query(user_query)\n",
    "    final = check(results)\n",
    "    for idx, query in enumerate(final, start=1):\n",
    "        print(f\"\\nQuery {idx}:\")\n",
    "        print(f\"SQL: {query}\")\n",
    "        try:\n",
    "            result_df = auto_fix_and_execute(query, spark, schema_details)\n",
    "            print(result_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to execute corrected query after several attempts: {e}\")\n",
    "\n",
    "execute()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b826154b-4a0e-4134-8f29-3058419485db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyhive import hive\n",
    "\n",
    "# conn = hive.Connection(\n",
    "#     host='localhost',\n",
    "#     port=10000,\n",
    "#     username='debasmith',\n",
    "#     database='default'\n",
    "# )\n",
    "# cursor = conn.cursor()\n",
    "# cursor.execute('SHOW TABLES')\n",
    "# print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1893d8bd-cad4-460e-ba75-0a8196bcb9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf05b606-9fe3-4474-a472-88770969e250",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2269694994.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    jupyter nbextension enable --py widgetsnbextension\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee285b8f-0ebf-4dfe-84b1-1f41aeaa79a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5745f34-72ee-4229-8c97-20c209d0a44c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
