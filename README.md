# ğŸ¤– DataWhisperer

DataWhisperer is an AI-powered analytics tool that lets you upload your data and get answers in plain English. Instead of writing complex SQL queries or code, you can simply ask questions, and the application will generate interactive visualizations and insights for you.

This application uses a local Large Language Model (LLM) via Ollama, PySpark for data processing, and Panel for the interactive web dashboard.

## âœ¨ Core Features

  * **File Upload:** Load your data directly into the app. Supports `.csv`, `.xlsx` and `.xls` files.
  * **Natural Language Queries:** Ask questions in plain English, like "What are the top 10 products by sales?" or "Show me the monthly revenue trend."
  * **AI-Powered SQL Generation:** Uses an LLM (e.g., `gemma3`) and LangChain to automatically convert your question into an optimized Spark SQL query.
  * **Smart Visualization:** A second AI agent analyzes your query and the SQL results to recommend the best visualization (bar chart, pie chart, line chart, KPI, etc.).
  * **Interactive Dashboards:** All charts are rendered with Plotly and are fully interactive (hover, zoom, pan).
  * **Spark-Powered:** Leverages the PySpark engine to handle data processing, enabling it to work with larger-than-memory datasets (though it's configured for local mode by default).
  * **Self-Correcting SQL:** If a generated SQL query fails, an AI-powered debugger chain attempts to fix the query and retry.

## ğŸš€ How It Works

DataWhisperer uses a multi-step AI chain to turn your question into a chart:

1.  **Upload:** You upload a data file. The app uses Pandas to quickly read the file and then creates a PySpark DataFrame and a temporary SQL view.
2.  **Ask:** You ask a question in the text box, e.g., "Count customers by region and show as a pie chart."
3.  **NL-to-SQL:** The app sends your question, the data's schema, and some sample rows to an LLM with a specialized "SQL Expert" prompt. The LLM generates a Spark SQL query.
4.  **Execute:** The generated SQL is run against the PySpark DataFrame.
5.  **Recommend Viz:** The original question, the SQL query, and a preview of the results are sent to a "Visualization Expert" AI prompt. This AI recommends the best chart type and configuration (e.g., `{"visualization_type": "pie", "title": "Customer Distribution by Region", ...}`). It also honors explicit requests like "show as a pie chart."
6.  **Render:** The `VisualizationEngine` uses Plotly and Panel to build and display the recommended interactive chart in the dashboard.

## ğŸ› ï¸ Tech Stack

  * **Data Processing:** `pyspark`, `pandas`, `pyarrow`
  * **Web Framework & UI:** `panel`
  * **Visualization:** `plotly`, `bokeh`
  * **LLM Integration:** `langchain`, `langchain-openai` (used to connect to any OpenAI-compatible API, including Ollama)
  * **File Handling:** `openpyxl`, `xlrd`
  * **Configuration:** `python-dotenv`

## Prerequisites: Run a Local LLM

This project is configured to use a local LLM served by **Ollama**.

1.  **Install Ollama:** Download and install it from [ollama.com](https://ollama.com/).
2.  **Pull a Model:** The project defaults to `gemma3`. You must pull this model first by running:
    ```sh
    ollama run gemma3
    ```
    (You can change this to another model, like `llama3` or `mistral`, in your configuration).

## âš™ï¸ Setup and Installation

1.  **Clone the repository:**

    ```sh
    git clone <your-repo-url>
    cd data-whisperer
    ```

2.  **Create and activate a virtual environment:**

    ```sh
    # Windows
    python -m venv myenv
    myenv\Scripts\activate.bat

    # macOS / Linux
    python3 -m venv myenv
    source myenv/bin/activate
    ```

3.  **Install the required dependencies:**

    ```sh
    pip install -r requirements.txt
    ```

4.  **Create a `.env` file:**
    Create a file named `.env` in the root of the project directory to configure your settings. This overrides the defaults in `src/config.py`.

    ```ini
    # .env
    # Set the LLM model to use (must be available in Ollama)
    DATAWHISPERER_LLM_MODEL=gemma3

    # Set the Ollama API endpoint
    DATAWHISPERER_LLM_URL=http://localhost:11434/v1

    # Set the port for the Panel app
    DATAWHISPERER_PORT=5007
    ```

## ğŸš€ Running the Application

1.  **Ensure Ollama is running** in the background.

2.  **Run the `main.py` script:**

    ```sh
    python main.py
    ```

3.  **Open the application:**
    The console will log the URL. By default, it is:
    **[http://localhost:5007](https://www.google.com/search?q=http://localhost:5007)**

You can now upload a file, ask questions, and see the results\!

## ğŸ“‚ Project Structure

```
/
â”œâ”€â”€ main.py             # Main application entry point that starts the Panel server
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ .env.example        # Example environment variables
â””â”€â”€ src/
    â”œâ”€â”€ config.py           # Dataclass for all application configuration
    â”œâ”€â”€ data/
    â”‚   â””â”€â”€ data_loader.py    # (Legacy/Alternative) Spark-native data loader
    â”œâ”€â”€ query/
    â”‚   â””â”€â”€ query_processor.py # Core AI logic: NL-to-SQL, SQL-to-Viz
    â”œâ”€â”€ ui/
    â”‚   â””â”€â”€ dashboard.py     # Panel dashboard UI components and layout
    â”œâ”€â”€ utils/
    â”‚   â””â”€â”€ logger.py        # Logging setup
    â””â”€â”€ visualization/
        â””â”€â”€ viz_engine.py      # Creates Plotly charts from data + config
```

## ğŸ”§ Configuration

All settings are managed in `src/config.py` and can be overridden with environment variables (as shown in the `.env` file setup).

Key settings you might want to change:

  * `llm_model`: The model to use from Ollama (e.g., `llama3`, `mistral`).
  * `llm_base_url`: The API endpoint for your LLM.
  * `port`: The port to run the web server on.
  * `supported_file_types`: List of allowed file extensions.
  * `max_retries`: Number of times the AI should try to fix a broken SQL query.
